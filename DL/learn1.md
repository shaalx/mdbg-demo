# 机器学习基本概念
 
## [线性回归模型 & 梯度下降法](http://blog.csdn.net/xiazdong/article/details/7950084)

 - 1. 线性回归的定义

 - 2. 单变量线性回归

 - 3. cost function：评价线性回归是否拟合训练集的方法，目标是使loss降低到阈值之内；

 - 4. 梯度下降：解决线性回归的方法之一，求最值问题（可能是局部最小值，跟初始值和learning rate相关）

 - 5. feature scaling：加快梯度下降执行速度的方法  标准化

 - 6.多变量线性回归

 - 7.learning rate: 步长，随J func改变

## [kernal func](https://www.zhihu.com/question/24627666)

核函数误解甚多，__核函数把低维空间映射到高维空间.__



模型表达就是给出输入和输出之间的函数关系式，当然这个函数是有前提假设的，里面可以含有参数。此时如果有许多训练样本的话，同样可以给出训练样本的平均相关的误差函数，一般该函数也称作是损失函数（Loss function）。我们的目标是求出模型表达中的参数，这是通过最小化损失函数来求得的。__一般最小化损失函数是通过梯度下降法（即先随机给出参数的一组值，然后更新参数，使每次更新后的结构都能够让损失函数变小，最终达到最小即可）。__在梯度下降法中，目标函数其实可以看做是参数的函数，因为给出了样本输入和输出值后，目标函数就只剩下参数部分了，这时可以把参数看做是自变量，则目标函数变成参数的函数了。梯度下降每次都是更新每个参数，且每个参数更新的形式是一样的，即用前一次该参数的值减掉学习率和目标函数对该参数的偏导数（如果只有1个参数的话，就是导数），为什么要这样做呢？通过取不同点处的参数可以看出，这样做恰好可以使原来的目标函数值变低，因此符合我们的要求（即求函数的最小值）。即使当学习速率固定(但不能太大)，梯度下降法也是可以收敛到一个局部最小点的，因为梯度值会越来越小，它和固定的学习率相乘后的积也会越来越小。在线性回归问题中我们就可以用梯度下降法来求回归方程中的参数。有时候该方法也称为批量梯度下降法，这里的批量指的是每一时候参数的更新使用到了所有的训练样本。

Vectorized implementation指的是矢量实现，由于实际问题中很多变量都是向量的，所有如果要把每个分量都写出来的话会很不方便，应该尽量写成矢量的形式。比如上面的梯度下降法的参数更新公式其实也是可以用矢量形式实现的。矢量形式的公式简单，且易用matlab编程。由于梯度下降法是按照梯度方向来收敛到极值的，如果输入样本各个维数的尺寸不同（即范围不同），则这些参数的构成的等高线不同的方向胖瘦不同，这样会导致参数的极值收敛速度极慢。因此在进行梯度下降法求参数前，需要先进行feature scaling这一项，__一般都是把样本中的各维变成0均值，即先减掉该维的均值，然后除以该变量的range。__

接下来就是学习率对梯度下降法的影响。如果学习速率过大，这每次迭代就有可能出现超调的现象，会在极值点两侧不断发散，最终损失函数的值是越变越大，而不是越来越小。在损失函数值——迭代次数的曲线图中，可以看到，该曲线是向上递增的。当然了，当学习速率过大时，还可能出现该曲线不断震荡的情形。如果学习速率太小，这该曲线下降得很慢，甚至在很多次迭代处曲线值保持不变。那到底该选什么值呢？这个一般是根据经验来选取的，比如从…0.0001,0.001,.0.01,0.1,1.0…这些参数中选，看那个参数使得损失值和迭代次数之间的函数曲线下降速度最快。